# ボルツマンマシン概説

**対象読者**: 理系大学院生（数学的基礎はあるが機械学習は初学者）

---

## 1. 機械学習における位置づけ

機械学習モデルは大きく**識別モデル**と**生成モデル**に分類される。

| 種類 | 目的 | 例 |
|------|------|-----|
| 識別モデル | 入力から出力を予測 $P(y\|x)$ | SVM、ロジスティック回帰、CNN |
| 生成モデル | データの分布自体を学習 $P(x)$ | VAE、GAN、**ボルツマンマシン** |

**ボルツマンマシン**は1985年にHintonとSejnowskiが提案した**確率的生成モデル**であり、統計力学の概念をニューラルネットワークに応用したものである。現代の深層学習の理論的基盤となった重要なモデルである。

---

## 2. なぜ「ボルツマン」なのか：統計力学との対応

物理学者ボルツマンは、熱平衡状態にある系がエネルギー $E$ の状態をとる確率が以下の分布に従うことを示した：

$$P(状態) = \frac{1}{Z} \exp\left(-\frac{E}{k_B T}\right)$$

- $k_B$：ボルツマン定数、$T$：温度、$Z$：分配関数（正規化定数）

**核心的な性質**：**低エネルギーの状態ほど高確率で実現する**

ボルツマンマシンはこの物理法則を「学習」に応用する：
- **訓練データに対応するパターン** → 低エネルギー（高確率）
- **訓練データにないパターン** → 高エネルギー（低確率）

学習とは、「良いパターン」のエネルギーを下げ、「悪いパターン」のエネルギーを上げる過程である。

---

## 3. 構造

### 3.1 ユニットと接続

ボルツマンマシンは**二値ユニット**（$s_i \in \{0, 1\}$）からなるネットワークである。

```
可視ユニット v：外部から観測可能（入力データを表現）
隠れユニット h：外部から観測不可（データの潜在的特徴を捉える）
```

**「隠れ」ユニットの役割**：機械学習では、データの背後にある「見えない構造」を学習することが重要である。例えば手書き数字画像では、ピクセル値（可視）の背後に「どの数字か」という潜在的な特徴（隠れ）がある。隠れユニットはこのような抽象的特徴を自動的に獲得する。

### 3.2 パラメータ

- $w_{ij}$：ユニット $i, j$ 間の**対称的な重み**（$w_{ij} = w_{ji}$）
- $b_i$：ユニット $i$ の**バイアス**（活性化のしやすさ）

---

## 4. エネルギー関数と確率分布

### 4.1 エネルギー関数

状態ベクトル $\mathbf{s} = (s_1, \ldots, s_n)$ に対するエネルギー：

$$E(\mathbf{s}) = -\sum_{i<j} w_{ij} s_i s_j - \sum_i b_i s_i = -\frac{1}{2}\mathbf{s}^\top \mathbf{W} \mathbf{s} - \mathbf{b}^\top \mathbf{s}$$

**直感的理解**：
- $w_{ij} > 0$ かつ $s_i = s_j = 1$ → エネルギー減少（協調すると安定）
- $w_{ij} < 0$ かつ $s_i = s_j = 1$ → エネルギー増加（協調すると不安定）

### 4.2 確率分布

状態 $\mathbf{s}$ の実現確率（温度 $T=1$ として）：

$$P(\mathbf{s}) = \frac{\exp(-E(\mathbf{s}))}{Z}, \quad Z = \sum_{\mathbf{s}'} \exp(-E(\mathbf{s}'))$$

分配関数 $Z$ は全 $2^n$ 状態の和であり、一般に計算が困難である。

### 4.3 可視・隠れユニットを分離した場合

$$E(\mathbf{v}, \mathbf{h}) = -\mathbf{b}^\top\mathbf{v} - \mathbf{c}^\top\mathbf{h} - \mathbf{v}^\top\mathbf{W}\mathbf{h}$$

観測データ $\mathbf{v}$ の確率は隠れ変数を周辺化して得る：$P(\mathbf{v}) = \sum_{\mathbf{h}} P(\mathbf{v}, \mathbf{h})$

---

## 5. 状態更新とサンプリング

### 5.1 確率的更新則

ユニット $i$ が活性化する確率：

$$P(s_i = 1 | \mathbf{s}_{-i}) = \sigma\left(\sum_j w_{ij} s_j + b_i\right), \quad \sigma(x) = \frac{1}{1+e^{-x}}$$

この更新を繰り返すと、系はボルツマン分布に収束する（**ギブスサンプリング**）。

### 5.2 温度の役割

温度パラメータ $T$ を導入すると $P(s_i=1) = \sigma\left(\frac{1}{T}(\sum_j w_{ij}s_j + b_i)\right)$：

| $T$ | 挙動 |
|-----|------|
| $T \to 0$ | 最低エネルギー状態のみ選択（決定論的） |
| $T = 1$ | 標準的な確率的動作 |
| $T \to \infty$ | 全状態が等確率（完全ランダム） |

---

## 6. 学習アルゴリズム

### 6.1 学習目標

訓練データの対数尤度を最大化：$\max_\theta \sum_{\mathbf{v} \in \text{data}} \log P(\mathbf{v}; \theta)$

### 6.2 勾配の構造

重み $w_{ij}$ に関する勾配は**2つの期待値の差**として表される：

$$\frac{\partial \log P(\mathbf{v})}{\partial w_{ij}} = \underbrace{\langle s_i s_j \rangle_{\text{data}}}_{\text{正相（Positive Phase）}} - \underbrace{\langle s_i s_j \rangle_{\text{model}}}_{\text{負相（Negative Phase）}}$$

- **正相**：データを可視層に固定し、隠れ層をサンプリングして相関を計算
- **負相**：モデルが自由に生成するパターンの相関を計算

**機械学習的解釈**：「データで見られる相関」と「モデルが現状で生成する相関」の差を縮める方向に重みを更新する。

### 6.3 完全ボルツマンマシンの困難さ

負相の計算には平衡状態までサンプリングが必要 → 計算コストが膨大（数千〜数万ステップ）

---

## 7. 制限ボルツマンマシン（RBM）

### 7.1 「制限」の意味

**同層内のユニット間に接続を持たない**という制約を課したモデル。

```
完全ボルツマンマシン          RBM（制限付き）
  h₁ ─── h₂ ─── h₃           h₁    h₂    h₃
   │╲   ╱│╲   ╱│              │╲   ╱│╲   ╱│
   │ ╲ ╱ │ ╲ ╱ │              │ ╲ ╱ │ ╲ ╱ │
   │ ╱ ╲ │ ╱ ╲ │              │ ╱ ╲ │ ╱ ╲ │
  v₁ ─── v₂ ─── v₃           v₁    v₂    v₃
  （層内接続あり）           （層内接続なし）
```

### 7.2 条件付き独立性がもたらす計算効率

層内接続がないため、条件付き分布が**完全に分解**する：

$$P(\mathbf{h}|\mathbf{v}) = \prod_j P(h_j|\mathbf{v}), \quad P(h_j=1|\mathbf{v}) = \sigma\left(c_j + \sum_i v_i w_{ij}\right)$$

$$P(\mathbf{v}|\mathbf{h}) = \prod_i P(v_i|\mathbf{h}), \quad P(v_i=1|\mathbf{h}) = \sigma\left(b_i + \sum_j w_{ij} h_j\right)$$

**実用上の意義**：層全体を一度に並列更新できるため、GPUとの相性が良い。

### 7.3 コントラスティブダイバージェンス（CD）

Hinton (2002) が提案した近似学習法。負相を少ないサンプリングステップで近似：

```
CD-k アルゴリズム（k=1が実用上よく使われる）
───────────────────────────────────────────
1. 正相：データ v⁽⁰⁾ → h⁽⁰⁾ をサンプル → positive = v⁽⁰⁾ ⊗ h⁽⁰⁾
2. 負相：h⁽⁰⁾ → v⁽¹⁾ → h⁽¹⁾（kステップ）→ negative = v⁽ᵏ⁾ ⊗ h⁽ᵏ⁾
3. 更新：ΔW = η(positive - negative)
```

厳密な勾配ではないが、実用上十分な精度で高速に学習できる。

---

## 8. 深層化と現代への影響

### 8.1 Deep Belief Network（DBN, 2006）

RBMを層ごとに貪欲に学習して積み重ねる手法。Hintonはこれにより「深いネットワークは学習可能」であることを示し、**ディープラーニング復興の契機**となった。

### 8.2 現代の機械学習における位置づけ

現在、ボルツマンマシン自体が実用で使われることは少ない（ReLU、Dropout、BatchNormなどの技術により事前学習なしでも深い網を学習可能になったため）。しかし以下の点で重要性を持つ：

| 観点 | 意義 |
|------|------|
| 理論的基盤 | VAE、GANなど現代の生成モデルの基礎概念 |
| エネルギーベース学習 | 最新研究でも活発に用いられる枠組み |
| 量子コンピューティング | 量子アニーリングとの親和性 |
| 人間行動モデリング | ボルツマン有理性（強化学習における行動選択確率）|

---

## 9. まとめ：核心的アイデア

1. **エネルギー関数で確率分布を定義**：低エネルギー = 高確率
2. **学習 = エネルギー地形の整形**：訓練データの谷を深くする
3. **正相と負相**：「データの統計」と「モデルの統計」を一致させる
4. **RBMの制約**：計算効率を劇的に改善する構造的工夫
5. **CDアルゴリズム**：近似により実用的な学習を実現

---

## 参考文献

- Hinton, G. E., & Sejnowski, T. J. (1986). *Learning and Relearning in Boltzmann Machines*
- Hinton, G. E. (2002). *Training Products of Experts by Minimizing Contrastive Divergence*
- Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). *A Fast Learning Algorithm for Deep Belief Nets*
- Salakhutdinov, R., & Hinton, G. (2009). *Deep Boltzmann Machines*
