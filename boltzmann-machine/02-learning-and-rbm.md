# ボルツマンマシン：学習とRBM編

**対象読者**: 理系大学院生（線形代数・確率論の基礎はあるが機械学習は初学者）

---

## 1. 学習アルゴリズム

### 1.1 学習の目標

ボルツマンマシンの学習目標は、訓練データの**対数尤度を最大化**すること：

$$\max_{\theta} \mathcal{L}(\theta) = \max_{\theta} \sum_{\mathbf{v} \in \text{data}} \log P(\mathbf{v}; \theta)$$

ここで $\theta = \{W, b, c, \ldots\}$ はモデルのパラメータ（重みとバイアス）。

**機械学習的な解釈**：モデルが訓練データを「高確率で生成する」ようにパラメータを調整する。

### 1.2 勾配の構造：正相と負相

重み $w_{ij}$ に関する勾配は、**2つの期待値の差**として表される：

$$\frac{\partial \log P(\mathbf{v})}{\partial w_{ij}} = \underbrace{\langle s_i s_j \rangle_{\text{data}}}_{\text{正相（Positive Phase）}} - \underbrace{\langle s_i s_j \rangle_{\text{model}}}_{\text{負相（Negative Phase）}}$$

#### 正相（Positive Phase）

$$\langle s_i s_j \rangle_{\text{data}}$$

- 訓練データを可視ユニットに固定（**クランプ**）
- 隠れユニットをサンプリング
- ユニット間の相関を計算

**意味**：「訓練データが与えられたとき、モデル内でどのような相関が生じるか」

#### 負相（Negative Phase）

$$\langle s_i s_j \rangle_{\text{model}}$$

- ユニットを固定せず自由に動かす
- 平衡状態に達するまでサンプリング
- モデルが自然に生成するパターンの相関を計算

**意味**：「モデルが自由に動いたとき、どのような相関を生成するか」

### 1.3 学習則の直感的理解

$$\Delta w_{ij} = \eta \left( \langle s_i s_j \rangle_{\text{data}} - \langle s_i s_j \rangle_{\text{model}} \right)$$

| 状況 | 重みの更新 |
|------|-----------|
| データの相関 > モデルの相関 | 重みを**増加** |
| データの相関 < モデルの相関 | 重みを**減少** |

**機械学習的解釈**：「データで見られる統計量」と「モデルが現状で生成する統計量」のギャップを埋める方向に学習する。これは**モーメントマッチング**の一種である。

### 1.4 完全ボルツマンマシンの学習の困難さ

#### 問題1：負相の計算コスト

負相の期待値を計算するには、モデルが平衡状態に達するまで十分なサンプリングが必要。

```
実際には数千〜数万ステップのギブスサンプリングが必要
→ 1回のパラメータ更新に非常に時間がかかる
```

#### 問題2：隠れユニット間の接続

隠れユニット同士が接続されていると、条件付き分布 $P(h_j | \mathbf{v}, \mathbf{h}_{-j})$ が他の隠れユニットに依存し、効率的なサンプリングができない。

これらの問題を解決するのが**制限ボルツマンマシン（RBM）** である。

---

## 2. 制限ボルツマンマシン（RBM）

### 2.1 「制限」の意味

**Restricted Boltzmann Machine (RBM)** は、ボルツマンマシンに以下の制約を課したモデル：

> **同じ層内のユニット間に接続を持たない**

```
完全ボルツマンマシン          RBM（制限付き）
  h₁ ─── h₂ ─── h₃           h₁    h₂    h₃
   │╲   ╱│╲   ╱│              │╲   ╱│╲   ╱│
   │ ╲ ╱ │ ╲ ╱ │              │ ╲ ╱ │ ╲ ╱ │
   │ ╱ ╲ │ ╱ ╲ │              │ ╱ ╲ │ ╱ ╲ │
  v₁ ─── v₂ ─── v₃           v₁    v₂    v₃

  層内接続あり                層内接続なし
  → 学習が困難                → 効率的な学習が可能
```

この構造は**二部グラフ（bipartite graph）** と呼ばれる。

### 2.2 RBMの構造とパラメータ

- **可視層**：$\mathbf{v} = (v_1, \ldots, v_m)$
- **隠れ層**：$\mathbf{h} = (h_1, \ldots, h_n)$
- **パラメータ**：
  - $\mathbf{W} \in \mathbb{R}^{m \times n}$：可視-隠れ間の重み行列
  - $\mathbf{b} \in \mathbb{R}^{m}$：可視バイアス
  - $\mathbf{c} \in \mathbb{R}^{n}$：隠れバイアス

### 2.3 エネルギー関数

$$E(\mathbf{v}, \mathbf{h}) = -\mathbf{b}^\top\mathbf{v} - \mathbf{c}^\top\mathbf{h} - \mathbf{v}^\top\mathbf{W}\mathbf{h}$$

成分表示：

$$E(\mathbf{v}, \mathbf{h}) = -\sum_i b_i v_i - \sum_j c_j h_j - \sum_{i,j} v_i w_{ij} h_j$$

### 2.4 条件付き独立性：RBMの最大の利点

層内接続がないため、**条件付き分布が完全に分解**する。

#### 可視が与えられたときの隠れ

$$P(\mathbf{h}|\mathbf{v}) = \prod_j P(h_j|\mathbf{v})$$

各隠れユニットは**独立に**サンプリング可能：

$$P(h_j = 1|\mathbf{v}) = \sigma\left(c_j + \sum_i v_i w_{ij}\right)$$

#### 隠れが与えられたときの可視

$$P(\mathbf{v}|\mathbf{h}) = \prod_i P(v_i|\mathbf{h})$$

$$P(v_i = 1|\mathbf{h}) = \sigma\left(b_i + \sum_j w_{ij} h_j\right)$$

### 2.5 計算効率の劇的な改善

| モデル | 1層の更新 | 理由 |
|--------|----------|------|
| 完全BM | $O(n)$ ステップ | ユニットを1つずつ順番に更新 |
| RBM | $O(1)$ ステップ | 層全体を一度に並列更新 |

これにより**GPU並列計算**との相性が非常に良くなる。

### 2.6 ブロックギブスサンプリング

RBMでは効率的な**ブロックギブスサンプリング**が可能：

```
1. v を固定 → h を全て並列にサンプル
2. h を固定 → v を全て並列にサンプル
3. 1-2 を繰り返す
```

これにより、サンプリングが大幅に高速化される。

### 2.7 自由エネルギー（閉形式で計算可能）

RBMでは自由エネルギーが**解析的に**計算できる：

$$F(\mathbf{v}) = -\mathbf{b}^\top\mathbf{v} - \sum_j \log\left(1 + \exp(c_j + \mathbf{W}_j^\top\mathbf{v})\right)$$

完全ボルツマンマシンではこの計算は困難（隠れユニット間の相互作用のため）。

---

## 3. コントラスティブダイバージェンス（CD）

### 3.1 アイデア

2002年にHintonが提案した**近似学習法**。

負相の完全な期待値を計算する代わりに、**少ないステップのサンプリング**で近似する。

### 3.2 CD-k アルゴリズム

```
CD-k アルゴリズム
──────────────────────────────────────────────────────
入力: 訓練データ v⁽⁰⁾

1. 正相:
   - v⁽⁰⁾ から P(h|v⁽⁰⁾) を計算
   - h⁽⁰⁾ をサンプリング（または確率値をそのまま使用）
   - positive = v⁽⁰⁾ ⊗ h⁽⁰⁾  （外積）

2. 負相（k ステップのギブスサンプリング）:
   for i = 1 to k:
     - h⁽ⁱ⁻¹⁾ から v⁽ⁱ⁾ をサンプリング
     - v⁽ⁱ⁾ から h⁽ⁱ⁾ をサンプリング
   - negative = v⁽ᵏ⁾ ⊗ h⁽ᵏ⁾

3. パラメータ更新:
   ΔW = η (positive - negative)
   Δb = η (v⁽⁰⁾ - v⁽ᵏ⁾)
   Δc = η (h⁽⁰⁾ - h⁽ᵏ⁾)
──────────────────────────────────────────────────────
```

### 3.3 なぜ動くのか

CD は厳密な勾配ではないが、**近似的に正しい方向**を指す。

```
真の勾配の方向
     ↑
    /
   /  ← CD-1 の方向（近似）
  /
 /___→ 完全ではないが「十分に良い」
```

直感的には：
- 正相は訓練データの統計量を正確に計算
- 負相は「モデルがデータからどれだけ離れようとするか」を短いステップで推定
- この差を縮める方向に更新すれば、学習は進む

### 3.4 CD-k の k の選び方

| k | 特徴 |
|---|------|
| 1 | 最速。多くの場合で十分 |
| 5-10 | より正確な勾配推定 |
| 大きい値 | 真の勾配に近づくが計算コスト増 |

実用上は **CD-1** がよく使われる。

### 3.5 Persistent Contrastive Divergence (PCD)

CDの改良版。サンプリングチェーンを各更新で捨てずに**持続**させる。

```
CD:  データ → サンプル → 捨てる → データ → サンプル → ...
                 ↑
            毎回リセット

PCD: データ → サンプル → 保持 → 継続サンプル → 保持 → ...
                         ↑
                  前回の状態から継続
```

**利点**：
- マルコフ連鎖が混合する時間を確保
- より良い負相のサンプルが得られる
- CD-1よりも精度が高い場合が多い

---

## 4. RBMの実装

### 4.1 学習コード例

```python
def cd1_update(v_data, W, b, c, lr):
    """CD-1による1回の更新"""
    batch_size = len(v_data)

    # 正相: データから隠れ層の確率を計算
    h_prob = sigmoid(v_data @ W + c)                    # P(h=1|v)
    h_sample = (np.random.rand(*h_prob.shape) < h_prob) # サンプリング
    positive = v_data.T @ h_prob / batch_size           # 正相の相関

    # 負相: 1ステップの再構成
    v_recon = sigmoid(h_sample @ W.T + b)               # P(v=1|h)
    h_recon = sigmoid(v_recon @ W + c)                  # P(h=1|v')
    negative = v_recon.T @ h_recon / batch_size         # 負相の相関

    # パラメータ更新
    W += lr * (positive - negative)
    b += lr * (v_data - v_recon).mean(axis=0)
    c += lr * (h_prob - h_recon).mean(axis=0)

    return W, b, c
```

### 4.2 重みの初期化

```python
# Xavier/Glorot 初期化
W = np.random.randn(n_visible, n_hidden) * np.sqrt(2.0 / (n_visible + n_hidden))
b = np.zeros(n_visible)
c = np.zeros(n_hidden)
```

### 4.3 学習のモニタリング

```python
# 再構成誤差（学習の進捗確認用、厳密な指標ではない）
recon_error = np.mean((v_data - v_recon)**2)

# 自由エネルギー
def free_energy(v, W, b, c):
    return -np.dot(v, b) - np.sum(np.log(1 + np.exp(c + v @ W)), axis=1)
```

### 4.4 学習のコツ

| 項目 | 推奨値・方法 |
|------|-------------|
| 学習率 | 0.001 〜 0.1 |
| バッチサイズ | 10 〜 100 |
| モメンタム | 0.5 → 0.9 に徐々に増加 |
| L2正則化 | $\Delta W = \eta(\text{positive} - \text{negative}) - \lambda W$ |
| スパース性 | 隠れユニットの平均活性を目標値（例：0.05）に近づける |

---

## 5. RBMの種類と拡張

### 5.1 Binary-Binary RBM

最も基本的な形式。両層とも二値（0/1）。

### 5.2 Gaussian-Binary RBM

可視層が**連続値**（実数値データ用）：

$$E(\mathbf{v}, \mathbf{h}) = \sum_i \frac{(v_i - b_i)^2}{2\sigma_i^2} - \sum_j c_j h_j - \sum_{i,j} \frac{v_i}{\sigma_i} w_{ij} h_j$$

画像などの連続値データに使用。

### 5.3 Softmax RBM

可視層が**カテゴリカル**（多クラス分類用）。単語の埋め込みなどに使用。

### 5.4 Conditional RBM (CRBM)

**時系列データ**用。過去の状態を条件として現在を生成：

$$E(\mathbf{v}^{(t)}, \mathbf{h}^{(t)} | \mathbf{v}^{(t-1)}) = \ldots$$

---

## 6. RBMの応用

| 応用 | 説明 |
|------|------|
| **次元削減** | 784次元（MNIST）→ 100次元（隠れ層活性）|
| **特徴学習** | 隠れユニットが自動的に有用な特徴を獲得 |
| **協調フィルタリング** | Netflix Prize で上位入賞（推薦システム）|
| **事前学習** | 深層ネットワークの初期化に使用 |

---

## まとめ：学習とRBM編のポイント

1. **学習 = 正相と負相の差を縮める**：データの統計とモデルの統計をマッチさせる
2. **RBMの「制限」**：層内接続を禁止することで条件付き独立性を獲得
3. **条件付き独立性**：$P(\mathbf{h}|\mathbf{v}) = \prod_j P(h_j|\mathbf{v})$ により並列計算が可能
4. **CDアルゴリズム**：少ないサンプリングステップで近似し、実用的な学習速度を実現
5. **RBMは実用的なビルディングブロック**：深層モデルの基盤として機能

---

[← 前へ: 基礎編](./01-foundations.md) | [次へ: 発展と応用 →](./03-advanced-and-applications.md)
