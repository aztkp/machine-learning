# 6. 深層ボルツマンマシン

## RBMを積み重ねる

単一のRBMでは表現力に限界があります。複数のRBMを積み重ねることで、より複雑なパターンを学習できます。

```
深層化のアプローチ:

┌─────────────┐
│  h³ (隠れ3) │
├─────────────┤
│  h² (隠れ2) │
├─────────────┤
│  h¹ (隠れ1) │
├─────────────┤
│  v  (可視)  │
└─────────────┘
```

## Deep Belief Network (DBN)

### 構造

DBNは**RBMを積み重ねた有向グラフモデル**です。

```
DBN の構造:

     h³
     ↕     ← 最上位2層: 無向結合 (RBM)
     h²
     ↓     ← これ以下: 有向結合
     h¹
     ↓
     v
```

- 最上位2層: 無向結合（RBM）
- その他の層: 有向結合（sigmoid belief network）

### 貪欲層ごと事前学習

DBNは**貪欲な層ごとの学習**で訓練します：

```
ステップ 1:
┌─────┐
│ h¹  │ ← RBMとして学習
├─────┤
│  v  │
└─────┘

ステップ 2:
┌─────┐
│ h²  │ ← h¹を可視層として
├─────┤    RBMを学習
│ h¹  │ (固定)
└─────┘

ステップ 3:
┌─────┐
│ h³  │ ← h²を可視層として
├─────┤    RBMを学習
│ h²  │ (固定)
└─────┘
```

### Hinton 2006 の画期的発見

> 各層のRBMを追加すると、データの対数尤度の**変分下界が改善**される

これが「ディープラーニング革命」の起爆剤となりました。

### DBNの使用方法

#### 1. 生成モデルとして

```
h³ からサンプル
     ↓
h² = σ(W₂ᵀh³)
     ↓
h¹ = σ(W₁ᵀh²)
     ↓
v  = σ(W₀ᵀh¹) ← 生成されたデータ
```

#### 2. 識別モデルの事前学習

```
事前学習後:
┌─────────┐
│ Softmax │ ← 分類層を追加
├─────────┤
│   h³    │
├─────────┤    ファインチューニング
│   h²    │    （バックプロパゲーション）
├─────────┤
│   h¹    │
├─────────┤
│   v     │
└─────────┘
```

## Deep Boltzmann Machine (DBM)

### DBNとの違い

DBMは全ての層が**無向結合**です：

```
DBN:                    DBM:
     h³                      h³
     ↕                       ↕
     h²                      h²
     ↓ (有向)                ↕ (無向)
     h¹                      h¹
     ↓ (有向)                ↕ (無向)
     v                       v
```

### DBMのエネルギー関数

3層DBMの場合：

$$E(\mathbf{v}, \mathbf{h}^1, \mathbf{h}^2) = -\mathbf{v}^T\mathbf{W}^1\mathbf{h}^1 - \mathbf{h}^{1T}\mathbf{W}^2\mathbf{h}^2 - \mathbf{b}^T\mathbf{v} - \mathbf{c}^{1T}\mathbf{h}^1 - \mathbf{c}^{2T}\mathbf{h}^2$$

### 条件付き分布

DBMでは隣接する層からの入力を考慮：

$$P(h^1_j = 1|\mathbf{v}, \mathbf{h}^2) = \sigma\left(c^1_j + \sum_i v_i W^1_{ij} + \sum_k h^2_k W^2_{jk}\right)$$

**両側から情報が来る**ため、RBMのような効率的なサンプリングができない。

### 平均場近似

DBMの推論には**平均場近似**を使用：

```
真の事後分布 P(h|v) を
分解可能な分布 Q(h) = Π Q(hⱼ) で近似

反復的に更新:
μ¹ⱼ ← σ(c¹ⱼ + Σᵢ vᵢW¹ᵢⱼ + Σₖ μ²ₖW²ⱼₖ)
μ²ₖ ← σ(c²ₖ + Σⱼ μ¹ⱼW²ⱼₖ + Σₗ μ³ₗW³ₖₗ)
...
収束するまで繰り返す
```

### DBMの学習

```
DBMの学習アルゴリズム:
─────────────────────────────────
1. 正相:
   - データを可視層に固定
   - 平均場推論で隠れ層の期待値を計算

2. 負相:
   - 全層でギブスサンプリング（または PCD）

3. 更新:
   ΔW = η (正相の相関 - 負相の相関)
```

### 事前学習の重要性

DBMは直接学習が困難なため、RBMによる事前学習が重要：

```
ステップ 1: 隣接層ペアをRBMとして学習
           （重みを2倍にするトリック使用）

ステップ 2: 事前学習した重みでDBMを初期化

ステップ 3: 変分学習でファインチューニング
```

## DBN vs DBM

| 特徴 | DBN | DBM |
|------|-----|-----|
| 結合 | 有向+無向 | 全て無向 |
| 推論 | フィードフォワード | 反復的（平均場） |
| 学習 | 比較的容易 | より困難 |
| 双方向情報 | なし | あり |
| 生成 | トップダウン | サンプリング |

## 深層モデルの利点

### 階層的特徴学習

```
層       学習する特徴（画像の場合）
────────────────────────────────
h³      顔全体、物体
 ↑
h²      目、鼻、口
 ↑
h¹      エッジ、コーナー
 ↑
v       ピクセル
```

### 表現効率

深い構造は、特定の関数を**指数関数的に効率よく**表現できる：

```
浅いネットワーク: O(2ⁿ) ユニット必要
深いネットワーク: O(n) ユニットで十分
（特定のクラスの関数に対して）
```

## 現代のディープラーニングとの関係

### 歴史的な役割

```
2006年: DBN による事前学習が有効と示される
        ↓
        「深いネットワークは学習できる」
        ↓
2010年代: バックプロパゲーションの改善
         - ReLU
         - Dropout
         - BatchNorm
         - 大規模データ
         - GPU
        ↓
現在:   事前学習なしでも深いネットワークを学習可能
```

### 残された価値

現在でもボルツマンマシン系モデルが有用な場面：

1. **生成モデル**: 真の確率モデルとしての性質
2. **不確実性の推定**: 確率的な予測
3. **量子コンピューティング**: 量子アニーリングとの親和性
4. **理論的理解**: 深層学習の基礎理論

## 実装例：シンプルなDBN

```python
class DBN:
    def __init__(self, layer_sizes):
        self.rbms = []
        for i in range(len(layer_sizes) - 1):
            rbm = RBM(layer_sizes[i], layer_sizes[i+1])
            self.rbms.append(rbm)

    def pretrain(self, data, epochs=10, lr=0.01):
        """層ごとの貪欲な事前学習"""
        current_input = data
        for i, rbm in enumerate(self.rbms):
            print(f"Training RBM layer {i+1}")
            rbm.train(current_input, epochs, lr)
            # 次の層への入力を計算
            current_input = rbm.transform(current_input)

    def generate(self, n_samples):
        """最上位層からサンプリングして生成"""
        # 最上位RBMからサンプル
        h = self.rbms[-1].sample_hidden(
            np.random.rand(n_samples, self.rbms[-1].n_visible)
        )
        # 下方向にプロパゲート
        for rbm in reversed(self.rbms):
            h = rbm.sample_visible(h)
        return h
```

---

[← 前へ: 制限ボルツマンマシン](./05-rbm.md) | [次へ: 応用例 →](./07-applications.md)
