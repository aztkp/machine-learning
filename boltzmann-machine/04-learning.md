# 4. 学習アルゴリズム

## 学習の目標

ボルツマンマシンの学習目標は、訓練データの**対数尤度を最大化**すること：

$$\max_{\theta} \mathcal{L}(\theta) = \max_{\theta} \sum_{\mathbf{v} \in \text{data}} \log P(\mathbf{v}; \theta)$$

## 勾配の2つの相

重み $w_{ij}$ に関する勾配：

$$\frac{\partial \mathcal{L}}{\partial w_{ij}} = \langle s_i s_j \rangle_{data} - \langle s_i s_j \rangle_{model}$$

### 正相（Positive Phase）

$$\langle s_i s_j \rangle_{data}$$

- データを可視ユニットに固定（クランプ）
- 隠れユニットをサンプリング
- ユニット間の相関を計算

### 負相（Negative Phase）

$$\langle s_i s_j \rangle_{model}$$

- ユニットを固定せず自由に動かす
- 平衡状態に達するまでサンプリング
- モデルが生成するパターンの相関を計算

### 学習則

$$\Delta w_{ij} = \eta \left( \langle s_i s_j \rangle_{data} - \langle s_i s_j \rangle_{model} \right)$$

**解釈**：
- データで見られる相関 > モデルの相関 → 重みを増加
- データで見られる相関 < モデルの相関 → 重みを減少

## 完全ボルツマンマシンの学習の困難さ

### 問題1: 負相の計算

負相の期待値を計算するには、モデルが平衡状態に達するまで十分なサンプリングが必要。

```
実際には数千〜数万ステップ必要なことも
→ 非常に遅い
```

### 問題2: 隠れユニット間の接続

隠れユニット同士が接続されていると、条件付き分布が複雑になり、効率的なサンプリングが困難。

## ギブスサンプリング

ボルツマン分布からサンプルを得る標準的な方法：

```
アルゴリズム: ギブスサンプリング
─────────────────────────────────
1. 状態をランダムに初期化
2. 以下を繰り返す:
   a. ユニット i をランダムに選ぶ
   b. 他のユニットの状態から P(sᵢ=1) を計算
   c. その確率に従って sᵢ を更新
3. 十分な反復後、サンプルを収集
```

### 収束の問題

```
初期状態 → ... → バーンイン期間 → ... → 平衡状態
          ↑                           ↑
      サンプル捨てる              サンプル収集
```

バーンイン期間をどれだけ取るかが問題。

## コントラスティブダイバージェンス（CD）

2002年にヒントンが提案した**近似学習法**。

### アイデア

負相の完全な期待値を計算する代わりに、**少ないステップのサンプリング**で近似。

### CD-k アルゴリズム

```
CD-k アルゴリズム（RBM用）
─────────────────────────────────
入力: 訓練データ v⁽⁰⁾

1. 正相:
   - v⁽⁰⁾ から h⁽⁰⁾ をサンプリング
   - positive = v⁽⁰⁾ ⊗ h⁽⁰⁾

2. 負相（k ステップのギブスサンプリング）:
   for i = 1 to k:
     - h⁽ⁱ⁻¹⁾ から v⁽ⁱ⁾ をサンプリング
     - v⁽ⁱ⁾ から h⁽ⁱ⁾ をサンプリング
   - negative = v⁽ᵏ⁾ ⊗ h⁽ᵏ⁾

3. パラメータ更新:
   ΔW = η (positive - negative)
```

### なぜ動くのか

CD は厳密な勾配ではないが、**近似的に正しい方向**を指す。

```
真の勾配の方向
     ↑
    /
   /  ← CD-1 の方向（近似）
  /
 /___→ CD の方向は完全ではないが
        「十分に良い」
```

### CD-k の k の選び方

| k | 特徴 |
|---|------|
| 1 | 最速、多くの場合で十分 |
| 5-10 | より正確な勾配 |
| 大きい | 真の勾配に近づくが遅い |

実用上は **CD-1** がよく使われます。

## Persistent Contrastive Divergence (PCD)

CDの改良版。サンプリングチェーンを各更新で捨てずに**持続**させる。

```
CD:  データ → サンプル → 捨てる → データ → ...

PCD: データ → サンプル → 保持 → 継続サンプル → ...
                         ↑
               前回の状態から継続
```

### 利点

- チェーンが混合する時間を確保
- より良い負相のサンプル
- CD-1よりも精度が高い場合が多い

## 学習のコツ

### 重みの初期化

```python
# 小さなランダム値で初期化
W = np.random.randn(n_visible, n_hidden) * 0.01
```

### 学習率

```
一般的な範囲: 0.001 〜 0.1
モメンタム: 0.5 → 0.9 に増加させる
```

### ミニバッチ学習

```python
for epoch in range(epochs):
    for batch in get_minibatches(data, batch_size=100):
        # CD-k で勾配計算
        # パラメータ更新
```

### 正則化

```
L2正則化（重み減衰）:
ΔW = η (positive - negative) - λW

スパース性:
隠れユニットの平均活性を目標値に近づける
```

## 学習の監視

### 再構成誤差

```
reconstruction_error = ||v - v'||²

v: 元のデータ
v': h をサンプリング後に再構成したもの
```

再構成誤差が下がれば学習が進んでいる**可能性**がある（厳密ではない）。

### 疑似対数尤度

真の対数尤度の計算は困難なので、近似指標を使用：

$$\text{PLL} = \sum_i \log P(v_i | v_{-i})$$

## アルゴリズムの比較

| アルゴリズム | 精度 | 速度 | 実装難易度 |
|-------------|------|------|-----------|
| 完全ギブス | 高 | 非常に遅い | 中 |
| CD-1 | 中 | 速い | 易 |
| CD-k | 中〜高 | やや遅い | 易 |
| PCD | 高 | 中 | 中 |

---

[← 前へ: エネルギー関数](./03-energy-function.md) | [次へ: 制限ボルツマンマシン →](./05-rbm.md)
