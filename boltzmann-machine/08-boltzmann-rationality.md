# 8. ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ã¨äººé–“-AIå”èª¿

## Overcooked-AI ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®æ´»ç”¨

[Overcooked-AI](https://github.com/HumanCompatibleAI/overcooked_ai) ã¯ã€äººé–“ã¨AIã®å”èª¿ã‚’ç ”ç©¶ã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒã§ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€**ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ï¼ˆBoltzmann Rationalityï¼‰** ãŒäººé–“ã®è¡Œå‹•ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚

## ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ã¨ã¯

### åŸºæœ¬æ¦‚å¿µ

ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ã¯ã€äººé–“ãŒ**å®Œå…¨ã«åˆç†çš„ã§ã¯ãªã„ãŒã€ã‚ˆã‚Šè‰¯ã„è¡Œå‹•ã‚’ã‚ˆã‚Šé«˜ã„ç¢ºç‡ã§é¸æŠã™ã‚‹**ã¨ã„ã†ä»®å®šã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

$$P(\text{è¡Œå‹• } a | \text{çŠ¶æ…‹ } s) \propto \exp(\beta \cdot Q(s, a))$$

- $Q(s, a)$: çŠ¶æ…‹ $s$ ã§è¡Œå‹• $a$ ã‚’å–ã£ãŸæ™‚ã®æœŸå¾…å ±é…¬
- $\beta$: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€†æ¸©åº¦ï¼‰
  - $\beta \to \infty$: å®Œå…¨ã«åˆç†çš„ï¼ˆæœ€é©è¡Œå‹•ã®ã¿ï¼‰
  - $\beta \to 0$: å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ 

### çµ±è¨ˆåŠ›å­¦ã¨ã®å¯¾å¿œ

| çµ±è¨ˆåŠ›å­¦ | äººé–“ã®è¡Œå‹•ãƒ¢ãƒ‡ãƒªãƒ³ã‚° |
|----------|---------------------|
| ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E$ | è² ã®å ±é…¬ $-R$ |
| æ¸©åº¦ $T$ | è¡Œå‹•ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ |
| ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒ | è¡Œå‹•é¸æŠç¢ºç‡ |

## å¾“æ¥ã®ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ã®é™ç•Œ

```
æœŸå¾…ã•ã‚Œã‚‹è¡Œå‹•: æœ€é©ã«è¿‘ã„è¡Œå‹•
å®Ÿéš›ã®äººé–“:     ç³»çµ±çš„ãªåã‚Šï¼ˆãƒã‚¤ã‚¢ã‚¹ï¼‰ã‚’æŒã¤

ä¾‹ï¼‰
- ã„ã¤ã‚‚åŒã˜çµŒè·¯ã‚’å¥½ã‚€
- ç‰¹å®šã®ã‚¿ã‚¤ãƒ—ã®è¡Œå‹•ã‚’é¿ã‘ã‚‹
- çŸ­æœŸçš„ãªå ±é…¬ã‚’éå¤§è©•ä¾¡
```

### å•é¡Œç‚¹

1. **ç³»çµ±çš„æº–æœ€é©æ€§ï¼ˆSystematic Suboptimalityï¼‰**
   - äººé–“ã¯å˜ã«ãƒ©ãƒ³ãƒ€ãƒ ãªãƒŸã‚¹ã‚’ã™ã‚‹ã®ã§ã¯ãªãã€ä¸€è²«ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã§æœ€é©ã‹ã‚‰é€¸è„±

2. **é©å¿œã®å›°é›£**
   - å¾“æ¥ã®ãƒœãƒ«ãƒ„ãƒãƒ³ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\beta$ ã®èª¿æ•´ã ã‘ã§ã¯å¯¾å¿œã§ããªã„

## ãƒœãƒ«ãƒ„ãƒãƒ³æ”¿ç­–åˆ†å¸ƒï¼ˆBPDï¼‰

### æ¦‚è¦

**Boltzmann Policy Distribution (BPD)** ã¯ã€ICLR 2022ã§ç™ºè¡¨ã•ã‚ŒãŸæ”¹è‰¯ç‰ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚

> Laidlaw, C., et al. "The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models" (ICLR 2022)

### ã‚¢ã‚¤ãƒ‡ã‚¢

æ”¿ç­–ç©ºé–“ä¸Šã«ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã‚’å®šç¾©ã—ã€**ãƒ™ã‚¤ã‚ºæ¨å®š**ã§äººé–“ã®è¡Œå‹•ã‚’å­¦ç¿’ï¼š

$$P(\pi | \text{è¦³æ¸¬}) \propto P(\text{è¦³æ¸¬} | \pi) \cdot P(\pi)$$

äº‹å‰åˆ†å¸ƒ $P(\pi)$ ã‚’ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã¨ã—ã¦è¨­å®šï¼š

$$P(\pi) \propto \exp\left(\beta \cdot \mathbb{E}\left[\sum_t R(s_t, a_t) | \pi\right]\right)$$

### BPDã®åˆ©ç‚¹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           å¾“æ¥ã®ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ å„æ™‚ç‚¹ã§ç‹¬ç«‹ã«è¡Œå‹•ã‚’äºˆæ¸¬                  â”‚
â”‚ â€¢ ç³»çµ±çš„ãªåã‚Šã‚’æ‰ãˆã‚‰ã‚Œãªã„                â”‚
â”‚ â€¢ æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã§èª¿æ•´                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BPDï¼ˆæ”¹è‰¯ç‰ˆï¼‰                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æ”¿ç­–å…¨ä½“ã¨ã—ã¦è¡Œå‹•ã‚’äºˆæ¸¬                  â”‚
â”‚ â€¢ è¦³æ¸¬ã‹ã‚‰ç³»çµ±çš„åã‚Šã‚’å­¦ç¿’                  â”‚
â”‚ â€¢ å˜ä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰é©å¿œå¯èƒ½                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Overcookedç’°å¢ƒã§ã®å®Ÿè£…

### ç’°å¢ƒã®æ¦‚è¦

```
Overcooked: 2äººã®æ–™ç†ã‚²ãƒ¼ãƒ 

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§…   ğŸ³   ğŸ³   ğŸ…          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚       [P1]     [P2]         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  ğŸ“¦        ğŸ½ï¸   ğŸ“¦          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç›®æ¨™: å”åŠ›ã—ã¦æ–™ç†ã‚’å®Œæˆã•ã›ã‚‹
èª²é¡Œ: ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ï¼ˆäººé–“ï¼‰ã®è¡Œå‹•ã‚’äºˆæ¸¬
```

### äººé–“ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ–¹æ³•

#### 1. è¡Œå‹•ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆBehavior Cloningï¼‰

```python
# äººé–“-äººé–“ã®ã‚²ãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
human_model = BehaviorCloning()
human_model.train(human_gameplay_data)
```

#### 2. ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ãƒ™ãƒ¼ã‚¹

```python
# å ±é…¬é–¢æ•°ã‹ã‚‰ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã‚’æ§‹ç¯‰
def boltzmann_policy(state, beta=1.0):
    q_values = estimate_q_values(state)
    probs = softmax(beta * q_values)
    return probs
```

#### 3. BPDï¼ˆæ”¹è‰¯ç‰ˆï¼‰

```python
# è¦³æ¸¬ã‹ã‚‰ãƒ™ã‚¤ã‚ºæ›´æ–°ã§æ”¿ç­–ã‚’æ¨å®š
from bpd.agents.bpd_trainer import BPDTrainer

bpd = BPDTrainer(config)
# ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä¸­ã®è¦³æ¸¬ã‹ã‚‰é©å¿œ
bpd.update_belief(observations)
```

## æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é›†å›£å­¦ç¿’ï¼ˆMEPï¼‰

### é–¢é€£ç ”ç©¶

**Maximum Entropy Population-Based Training (MEP)** ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆäººé–“-AIå”èª¿ã®ãŸã‚ã®æ‰‹æ³•ã§ã™ã€‚

> "Maximum Entropy Population-Based Training for Zero-Shot Human-AI Coordination" (AAAI 2023)

### ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã®é–¢ä¿‚

æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ï¼šåˆ¶ç´„ä¸‹ã§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨ã€ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

$$\max H(P) = -\sum_x P(x) \log P(x)$$
$$\text{subject to } \mathbb{E}[f(x)] = c$$

è§£ï¼š$P(x) \propto \exp(-\lambda f(x))$ ï¼ˆãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒï¼‰

### MEPã®ä»•çµ„ã¿

```
é›†å›£ï¼ˆPopulationï¼‰å†…ã«å¤šæ§˜ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä¿æŒ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent 1  Agent 2  Agent 3  ... Agent N â”‚
â”‚    â†“        â†“        â†“           â†“      â”‚
â”‚  [ç•°ãªã‚‹æˆ¦ç•¥ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç¾¤]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹ã§å¤šæ§˜æ€§ä¿ƒé€²
                    â†“
        æœªçŸ¥ã®äººé–“ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚‚å¯¾å¿œå¯èƒ½
```

## å®Ÿè·µçš„ãªå®Ÿè£…

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# BPD ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install boltzmann-policy-distribution

# Overcookedç’°å¢ƒ
pip install overcooked-ai
```

### åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹

```python
import numpy as np
from overcooked_ai_py.mdp.overcooked_mdp import OvercookedMDP
from overcooked_ai_py.agents.agent import AgentFromPolicy

# ç’°å¢ƒã®ä½œæˆ
mdp = OvercookedMDP.from_layout_name("cramped_room")

# ãƒœãƒ«ãƒ„ãƒãƒ³æ”¿ç­–ã®å®Ÿè£…
class BoltzmannAgent:
    def __init__(self, beta=1.0):
        self.beta = beta

    def action(self, state, q_values):
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§è¡Œå‹•ç¢ºç‡ã‚’è¨ˆç®—
        exp_q = np.exp(self.beta * q_values)
        probs = exp_q / np.sum(exp_q)
        return np.random.choice(len(probs), p=probs)
```

## ãƒœãƒ«ãƒ„ãƒãƒ³ãƒã‚·ãƒ³ã¨ã®é–¢ä¿‚

### å…±é€šç‚¹

| ãƒœãƒ«ãƒ„ãƒãƒ³ãƒã‚·ãƒ³ | ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ |
|-----------------|-----------------|
| ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’ç¢ºç‡çš„ã«é¸æŠ | è¡Œå‹•ã‚’ç¢ºç‡çš„ã«é¸æŠ |
| ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– | å ±é…¬æœ€å¤§åŒ– |
| æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | åˆç†æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| ã‚®ãƒ–ã‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | è¡Œå‹•ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |

### é•ã„

- **ãƒœãƒ«ãƒ„ãƒãƒ³ãƒã‚·ãƒ³**: é™çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ï¼ˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼‰
- **ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§**: å‹•çš„ãªæ„æ€æ±ºå®šï¼ˆè¡Œå‹•ãƒ¢ãƒ‡ãƒ«ï¼‰

### çµ±ä¸€çš„ãªè¦–ç‚¹

ä¸¡è€…ã¨ã‚‚**ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹**ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒã¨ã„ã†å…±é€šã®æ•°å­¦çš„åŸºç›¤ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

```
ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ ãƒœãƒ«ãƒ„ãƒãƒ³ãƒã‚·ãƒ³ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ï¼‰
â”‚   â””â”€â”€ çŠ¶æ…‹ã®ç¢ºç‡: P(s) âˆ exp(-E(s)/T)
â”‚
â””â”€â”€ ãƒœãƒ«ãƒ„ãƒãƒ³æœ‰ç†æ€§ï¼ˆæ„æ€æ±ºå®šï¼‰
    â””â”€â”€ è¡Œå‹•ã®ç¢ºç‡: P(a) âˆ exp(Î²Q(s,a))
```

## å‚è€ƒæ–‡çŒ®

1. Laidlaw, C., et al. "The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models" (ICLR 2022)
2. Carroll, M., et al. "On the Utility of Learning about Humans for Human-AI Coordination" (NeurIPS 2019)
3. Zhao, R., et al. "Maximum Entropy Population-Based Training for Zero-Shot Human-AI Coordination" (AAAI 2023)

## é–¢é€£ãƒªãƒã‚¸ãƒˆãƒª

- [Overcooked-AI](https://github.com/HumanCompatibleAI/overcooked_ai) - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒ
- [Human-Aware RL](https://github.com/HumanCompatibleAI/human_aware_rl) - äººé–“è€ƒæ…®å‹å¼·åŒ–å­¦ç¿’
- [Boltzmann Policy Distribution](https://github.com/cassidylaidlaw/boltzmann-policy-distribution) - BPDå®Ÿè£…

---

[â† å‰ã¸: å¿œç”¨ä¾‹](./07-applications.md) | [ãƒˆãƒƒãƒ—ã¸æˆ»ã‚‹ â†’](./README.md)
