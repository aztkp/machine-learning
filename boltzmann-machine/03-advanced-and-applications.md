# ボルツマンマシン：発展と応用編

**対象読者**: 理系大学院生（線形代数・確率論の基礎はあるが機械学習は初学者）

---

## 1. 深層ボルツマンマシン

### 1.1 なぜ深層化するのか

単一のRBMでは表現力に限界がある。複数の層を積み重ねることで：

- **階層的な特徴表現**を学習できる
- より**複雑なデータ分布**をモデル化できる
- **抽象度の異なる特徴**を段階的に獲得できる

```
層       学習する特徴（画像の場合）
────────────────────────────────
h³      顔全体、物体
 ↑
h²      目、鼻、口
 ↑
h¹      エッジ、コーナー
 ↑
v       ピクセル
```

深層化には主に2つのアプローチがある：**Deep Belief Network (DBN)** と **Deep Boltzmann Machine (DBM)**。

---

## 2. Deep Belief Network (DBN)

### 2.1 構造

DBNは**RBMを積み重ねた有向グラフモデル**である。

```
DBN の構造:

     h³
     ↕     ← 最上位2層: 無向結合 (RBM)
     h²
     ↓     ← これ以下: 有向結合 (sigmoid belief network)
     h¹
     ↓
     v
```

- 最上位2層：無向結合（RBMとして機能）
- その他の層：上から下への有向結合

### 2.2 貪欲な層ごと事前学習

DBNは**貪欲（greedy）な層ごとの学習**で訓練する。

```
ステップ 1: 第1層を学習
┌─────┐
│ h¹  │ ← vとh¹をRBMとして学習
├─────┤
│  v  │
└─────┘

ステップ 2: 第2層を学習
┌─────┐
│ h²  │ ← h¹の活性を可視層として
├─────┤    新しいRBMを学習
│ h¹  │ (重みは固定)
└─────┘

ステップ 3: 第3層を学習
┌─────┐
│ h³  │ ← h²の活性を可視層として
├─────┤    新しいRBMを学習
│ h²  │ (重みは固定)
└─────┘
```

**なぜこれが有効か（Hinton 2006の画期的発見）**：

> 各層のRBMを追加すると、データの対数尤度の**変分下界が単調に改善**される

この理論的保証が「深いネットワークは学習可能」という認識を広め、**ディープラーニング復興の契機**となった。

### 2.3 DBNの使用方法

#### 生成モデルとして

```
h³ から MCMC でサンプル
     ↓
h² = σ(W₂ᵀ h³)  （上から下へ決定論的に伝播）
     ↓
h¹ = σ(W₁ᵀ h²)
     ↓
v  = σ(W₀ᵀ h¹) ← 生成されたデータ
```

#### 識別モデルの事前学習として

```
事前学習後の構造:
┌─────────┐
│ Softmax │ ← 分類層を追加
├─────────┤
│   h³    │
├─────────┤    ← バックプロパゲーションで
│   h²    │      ファインチューニング
├─────────┤
│   h¹    │
├─────────┤
│   v     │
└─────────┘
```

**機械学習的意義**：ラベルなしデータで特徴表現を学習（教師なし事前学習）し、少量のラベル付きデータでファインチューニングする。これは**半教師あり学習**の先駆的手法である。

---

## 3. Deep Boltzmann Machine (DBM)

### 3.1 DBNとの違い

DBMは全ての層が**無向結合**である。

```
DBN:                    DBM:
     h³                      h³
     ↕                       ↕
     h²                      h²
     ↓ (有向)                ↕ (無向)
     h¹                      h¹
     ↓ (有向)                ↕ (無向)
     v                       v
```

### 3.2 エネルギー関数

3層DBMの場合：

$$E(\mathbf{v}, \mathbf{h}^1, \mathbf{h}^2) = -\mathbf{v}^\top\mathbf{W}^1\mathbf{h}^1 - \mathbf{h}^{1\top}\mathbf{W}^2\mathbf{h}^2 - \mathbf{b}^\top\mathbf{v} - \mathbf{c}^{1\top}\mathbf{h}^1 - \mathbf{c}^{2\top}\mathbf{h}^2$$

### 3.3 双方向の情報伝播

DBMでは各隠れ層が**両側の層から入力を受ける**：

$$P(h^1_j = 1|\mathbf{v}, \mathbf{h}^2) = \sigma\left(c^1_j + \sum_i v_i W^1_{ij} + \sum_k h^2_k W^2_{jk}\right)$$

これにより、推論時に上位層の情報も活用できる（DBNでは下から上への一方向のみ）。

### 3.4 平均場近似による推論

DBMの推論は計算が困難なため、**平均場近似（mean-field approximation）** を使用：

$$Q(\mathbf{h}) = \prod_j Q(h_j) \approx P(\mathbf{h}|\mathbf{v})$$

各隠れユニットの期待値 $\mu_j = \mathbb{E}_Q[h_j]$ を反復的に更新：

```
repeat until convergence:
    μ¹ⱼ ← σ(c¹ⱼ + Σᵢ vᵢW¹ᵢⱼ + Σₖ μ²ₖW²ⱼₖ)
    μ²ₖ ← σ(c²ₖ + Σⱼ μ¹ⱼW²ⱼₖ + Σₗ μ³ₗW³ₖₗ)
    ...
```

### 3.5 DBN vs DBM

| 特徴 | DBN | DBM |
|------|-----|-----|
| 結合 | 有向+無向 | 全て無向 |
| 推論 | フィードフォワード（高速） | 反復的・平均場（遅い） |
| 学習 | 比較的容易 | より困難 |
| 双方向情報 | なし | あり |
| 生成 | トップダウン | MCMCサンプリング |

---

## 4. 現代のディープラーニングとの関係

### 4.1 歴史的役割

```
2006年: DBN による事前学習が深いネットワークの学習に有効と示される
        ↓
        「深いネットワークは学習できる」という認識の転換
        ↓
2010年代: バックプロパゲーション技術の改善
         - ReLU活性化関数
         - Dropout正則化
         - Batch Normalization
         - 大規模データセット
         - GPU計算
        ↓
現在:   事前学習なしでも深いネットワークを学習可能
        （ボルツマンマシン系の事前学習は必須ではなくなった）
```

### 4.2 現代における意義

ボルツマンマシン自体が実用で使われることは少なくなったが、以下の点で重要性を持つ：

| 観点 | 意義 |
|------|------|
| **理論的基盤** | VAE、GANなど現代の生成モデルの概念的基礎 |
| **エネルギーベース学習** | 最新研究でも活発に用いられる枠組み（EBM） |
| **量子コンピューティング** | 量子アニーリングとの親和性（D-Waveなど） |
| **確率的推論** | 不確実性を扱うモデルの基礎理論 |

---

## 5. 応用例

### 5.1 画像処理

#### 画像生成・再構成

```
ノイズ入力 → [RBM] → h → [RBM] → クリーンな出力

学習したRBMは「正しいパターン」を記憶しているため、
ノイズのある入力から元の画像を復元できる
```

#### 特徴学習

MNISTで学習したRBMの重みを可視化すると、数字の部分パターン（エッジ、ストローク）が現れる。これは隠れユニットが自動的に有用な特徴を獲得したことを示す。

### 5.2 推薦システム

Netflix Prize (2006) でRBMベースのモデルが上位入賞。

```
ユーザー×映画の評価行列:

          映画1  映画2  映画3  映画4
ユーザーA   5     ?      3     ?
ユーザーB   ?     4      ?     2

  ↓ RBM で学習

「?」の部分を予測（協調フィルタリング）
```

### 5.3 異常検知

正常データでRBMを学習すると、異常データは高い再構成誤差を示す。

```python
def detect_anomaly(rbm, data, threshold):
    reconstruction = rbm.reconstruct(data)
    error = np.mean((data - reconstruction)**2, axis=1)
    return error > threshold
```

応用分野：製造業の品質管理、ネットワーク侵入検知、金融詐欺検出

### 5.4 自然言語処理

- **単語埋め込み**：高次元のone-hot表現から低次元の分散表現を学習
- **トピックモデリング**：文書のトピック分布をRBMで学習

---

## 6. ボルツマン有理性と強化学習

### 6.1 ボルツマン有理性とは

ボルツマンマシンの考え方は、**人間の意思決定モデリング**にも応用される。

**ボルツマン有理性（Boltzmann Rationality）**：人間は完全に合理的ではないが、より良い行動をより高い確率で選択する

$$P(\text{行動 } a | \text{状態 } s) \propto \exp(\beta \cdot Q(s, a))$$

- $Q(s, a)$：状態 $s$ で行動 $a$ を取った時の期待報酬（Q値）
- $\beta$：逆温度パラメータ
  - $\beta \to \infty$：完全に合理的（最適行動のみ選択）
  - $\beta \to 0$：完全にランダム

### 6.2 統計力学との対応

| 統計力学 | 人間の行動モデリング |
|----------|---------------------|
| エネルギー $E$ | 負の報酬 $-Q$ |
| 温度 $T$ | 行動のランダム性 $1/\beta$ |
| ボルツマン分布 | 行動選択確率 |

### 6.3 人間-AI協調への応用

[Overcooked-AI](https://github.com/HumanCompatibleAI/overcooked_ai) などの研究では、ボルツマン有理性に基づいて人間の行動を予測し、AIが協調的に行動するモデルが開発されている。

#### Boltzmann Policy Distribution (BPD)

従来のボルツマン有理性の限界（系統的な偏りを捉えられない）を克服するため、政策空間上にボルツマン分布を定義：

$$P(\pi) \propto \exp\left(\beta \cdot \mathbb{E}\left[\sum_t R(s_t, a_t) | \pi\right]\right)$$

観測からベイズ推定で人間の政策を学習し、単一エピソードからでも適応可能。

### 6.4 ボルツマンマシンとボルツマン有理性の関係

| 項目 | ボルツマンマシン | ボルツマン有理性 |
|------|-----------------|-----------------|
| 対象 | パターン（静的） | 行動（動的） |
| 確率の対象 | ノードの状態 | 行動の選択 |
| 最適化 | エネルギー最小化 | 報酬最大化 |
| 温度の意味 | 探索のランダム性 | 合理性の度合い |

両者とも**エネルギーベースの確率モデル**であり、ボルツマン分布という共通の数学的基盤を持つ。

---

## 7. 現代の代替手法との比較

| タスク | ボルツマンマシン系 | 現代の主流手法 |
|--------|-------------------|---------------|
| 画像生成 | DBN/DBM | GAN, Diffusion Models |
| 特徴学習 | RBM | 自己教師あり学習 (SimCLR, BYOL) |
| 推薦システム | RBM | Matrix Factorization, DNN |
| 異常検知 | RBM | VAE, Autoencoder |
| 事前学習 | DBN | Transformer (BERT, GPT) |
| 意思決定 | ボルツマン有理性 | Softmax政策 (強化学習) |

---

## まとめ：発展と応用編のポイント

1. **DBN**：RBMを層ごとに貪欲学習で積み重ね、ディープラーニング復興の契機となった
2. **DBM**：全層が無向結合で双方向の情報伝播が可能だが、学習が困難
3. **歴史的意義**：「深いネットワークは学習可能」を実証し、現代の深層学習への道を開いた
4. **現代での位置づけ**：直接使われることは少ないが、理論的基盤として重要
5. **ボルツマン有理性**：人間の確率的意思決定をモデル化し、人間-AI協調に応用

---

## 参考文献

### ボルツマンマシンの基礎
- Hinton, G. E., & Sejnowski, T. J. (1986). *Learning and Relearning in Boltzmann Machines*
- Hinton, G. E. (2002). *Training Products of Experts by Minimizing Contrastive Divergence*

### 深層ボルツマンマシン
- Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). *A Fast Learning Algorithm for Deep Belief Nets*
- Salakhutdinov, R., & Hinton, G. (2009). *Deep Boltzmann Machines*

### ボルツマン有理性と人間-AI協調
- Carroll, M., et al. (2019). *On the Utility of Learning about Humans for Human-AI Coordination*
- Laidlaw, C., et al. (2022). *The Boltzmann Policy Distribution* (ICLR 2022)
- Zhao, R., et al. (2023). *Maximum Entropy Population-Based Training for Zero-Shot Human-AI Coordination* (AAAI 2023)

---

[← 前へ: 学習とRBM](./02-learning-and-rbm.md) | [概要へ戻る →](./boltzmann-machine-overview.md)
